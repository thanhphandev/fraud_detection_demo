"""
Module tr·ª±c quan h√≥a k·∫øt qu·∫£ cho d·ª± √°n ph√°t hi·ªán gian l·∫≠n th·∫ª t√≠n d·ª•ng.

Module n√†y cung c·∫•p c√°c h√†m ƒë·ªÉ:
- V·∫Ω confusion matrix
- T·∫°o b·∫£ng so s√°nh c√°c m√¥ h√¨nh
- Tr·ª±c quan h√≥a c√°c metrics
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import streamlit as st


def plot_confusion_matrix(cm, model_name, figsize=(8, 6)):
    """
    V·∫Ω confusion matrix d∆∞·ªõi d·∫°ng heatmap.
    
    Args:
        cm (np.array): Confusion matrix (2x2)
        model_name (str): T√™n m√¥ h√¨nh
        figsize (tuple): K√≠ch th∆∞·ªõc figure
        
    Returns:
        matplotlib.figure.Figure: Figure object
    """
    fig, ax = plt.subplots(figsize=figsize)
    
    # T·∫°o heatmap
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        cbar=True,
        square=True,
        ax=ax,
        annot_kws={'size': 14, 'weight': 'bold'}
    )
    
    # Thi·∫øt l·∫≠p labels
    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
    ax.set_ylabel('True Label', fontsize=12, fontweight='bold')
    ax.set_title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)
    
    # Thi·∫øt l·∫≠p tick labels
    ax.set_xticklabels(['Normal (0)', 'Fraud (1)'], fontsize=11)
    ax.set_yticklabels(['Normal (0)', 'Fraud (1)'], fontsize=11, rotation=0)
    
    # Th√™m ch√∫ th√≠ch
    tn, fp, fn, tp = cm.ravel()
    textstr = f'TN: {tn}  |  FP: {fp}\nFN: {fn}  |  TP: {tp}'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,
            verticalalignment='top', bbox=props)
    
    plt.tight_layout()
    return fig


def create_metrics_dataframe(models):
    """
    T·∫°o DataFrame ch·ª©a c√°c metrics c·ªßa t·∫•t c·∫£ c√°c m√¥ h√¨nh.
    
    Args:
        models (list): Danh s√°ch c√°c FraudDetectionModel ƒë√£ ƒë∆∞·ª£c ƒë√°nh gi√°
        
    Returns:
        pd.DataFrame: DataFrame ch·ª©a metrics
    """
    data = []
    
    for model in models:
        metrics = model.metrics
        data.append({
            'T√™n m√¥ h√¨nh': metrics['model_name'],
            'True Positive (TP)': metrics['true_positive'],
            'False Positive (FP)': metrics['false_positive'],
            'True Negative (TN)': metrics['true_negative'],
            'False Negative (FN)': metrics['false_negative'],
            'Accuracy (%)': f"{metrics['accuracy'] * 100:.2f}",
            'Precision (%)': f"{metrics['precision'] * 100:.2f}",
            'Recall (%)': f"{metrics['recall'] * 100:.2f}",
            'F1-Score (%)': f"{metrics['f1_score'] * 100:.2f}"
        })
    
    df = pd.DataFrame(data)
    return df


def display_metrics_summary(model):
    """
    Hi·ªÉn th·ªã t√≥m t·∫Øt metrics c·ªßa m·ªôt m√¥ h√¨nh d∆∞·ªõi d·∫°ng cards.
    
    Args:
        model: FraudDetectionModel ƒë√£ ƒë∆∞·ª£c ƒë√°nh gi√°
    """
    metrics = model.metrics
    
    # T·∫°o 4 c·ªôt cho c√°c metrics ch√≠nh
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="‚úÖ True Positive (TP)",
            value=metrics['true_positive'],
            help="S·ªë giao d·ªãch gian l·∫≠n ƒë∆∞·ª£c ph√°t hi·ªán ƒë√∫ng"
        )
    
    with col2:
        st.metric(
            label="‚ùå False Positive (FP)",
            value=metrics['false_positive'],
            help="S·ªë giao d·ªãch h·ª£p ph√°p b·ªã nh·∫≠n di·ªán nh·∫ßm l√† gian l·∫≠n"
        )
    
    with col3:
        st.metric(
            label="Accuracy",
            value=f"{metrics['accuracy'] * 100:.2f}%",
            help="ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ c·ªßa m√¥ h√¨nh"
        )
    
    with col4:
        st.metric(
            label="F1-Score",
            value=f"{metrics['f1_score'] * 100:.2f}%",
            help="ƒêi·ªÉm c√¢n b·∫±ng gi·ªØa Precision v√† Recall"
        )


def plot_comparison_chart(models, metric='f1_score'):
    """
    V·∫Ω bi·ªÉu ƒë·ªì so s√°nh c√°c m√¥ h√¨nh theo m·ªôt metric c·ª• th·ªÉ.
    
    Args:
        models (list): Danh s√°ch c√°c FraudDetectionModel
        metric (str): T√™n metric c·∫ßn so s√°nh
        
    Returns:
        matplotlib.figure.Figure: Figure object
    """
    model_names = [m.model_name for m in models]
    metric_values = [m.metrics[metric] * 100 for m in models]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # T·∫°o bar chart
    bars = ax.bar(model_names, metric_values, color='skyblue', edgecolor='navy', linewidth=1.5)
    
    # Th√™m gi√° tr·ªã l√™n m·ªói bar
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}%',
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # Thi·∫øt l·∫≠p labels v√† title
    metric_labels = {
        'accuracy': 'Accuracy',
        'precision': 'Precision',
        'recall': 'Recall',
        'f1_score': 'F1-Score'
    }
    
    ax.set_ylabel(f'{metric_labels.get(metric, metric)} (%)', fontsize=12, fontweight='bold')
    ax.set_xlabel('M√¥ h√¨nh', fontsize=12, fontweight='bold')
    ax.set_title(f'So s√°nh {metric_labels.get(metric, metric)} gi·ªØa c√°c m√¥ h√¨nh', 
                 fontsize=14, fontweight='bold', pad=20)
    
    # Xoay labels tr·ª•c x n·∫øu c·∫ßn
    plt.xticks(rotation=15, ha='right')
    
    # Th√™m grid
    ax.yaxis.grid(True, linestyle='--', alpha=0.7)
    ax.set_axisbelow(True)
    
    plt.tight_layout()
    return fig


def display_data_info(data_info, method_info):
    """
    Hi·ªÉn th·ªã th√¥ng tin v·ªÅ d·ªØ li·ªáu g·ªëc v√† sau x·ª≠ l√Ω.
    
    Args:
        data_info (dict): Th√¥ng tin v·ªÅ d·ªØ li·ªáu g·ªëc
        method_info (dict): Th√¥ng tin v·ªÅ ph∆∞∆°ng ph√°p x·ª≠ l√Ω
    """
    st.subheader("Th√¥ng tin d·ªØ li·ªáu")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**D·ªØ li·ªáu g·ªëc:**")
        st.write(f"- T·ªïng s·ªë giao d·ªãch: {data_info['total_transactions']:,}")
        st.write(f"- Giao d·ªãch h·ª£p ph√°p: {data_info['normal_transactions']:,}")
        st.write(f"- Giao d·ªãch gian l·∫≠n: {data_info['fraud_transactions']:,}")
        st.write(f"- T·ª∑ l·ªá gian l·∫≠n: {data_info['fraud_percentage']:.3f}%")
    
    with col2:
        st.markdown(f"**Sau x·ª≠ l√Ω ({method_info['method']}):**")
        st.write(f"- T·ªïng s·ªë m·∫´u hu·∫•n luy·ªán: {method_info['resampled_total']:,}")
        st.write(f"- M·∫´u h·ª£p ph√°p: {method_info['resampled_normal']:,}")
        st.write(f"- M·∫´u gian l·∫≠n: {method_info['resampled_fraud']:,}")
        st.write(f"- T·ª∑ l·ªá gian l·∫≠n: {method_info['resampled_fraud_percentage']:.2f}%")


def get_recommendation(models, method_name):
    """
    Hi·ªÉn th·ªã khuy·∫øn ngh·ªã tr·ª±c quan d·ª±a tr√™n k·∫øt qu·∫£ c√°c m√¥ h√¨nh.
    
    Args:
        models (list): Danh s√°ch c√°c FraudDetectionModel
        method_name (str): T√™n ph∆∞∆°ng ph√°p x·ª≠ l√Ω d·ªØ li·ªáu
    """
    if not models:
        st.warning("Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c ƒë√°nh gi√°.")
        return
    
    # T√¨m c√°c m√¥ h√¨nh t·ªët nh·∫•t theo t·ª´ng ti√™u ch√≠
    best_model = max(models, key=lambda m: m.metrics['f1_score'])
    best_tp_model = max(models, key=lambda m: m.metrics['true_positive'])
    best_fp_model = min(models, key=lambda m: m.metrics['false_positive'])
    
    st.subheader("üìä Ph√¢n t√≠ch & Khuy·∫øn ngh·ªã")
    
    # Hi·ªÉn th·ªã ph∆∞∆°ng ph√°p x·ª≠ l√Ω
    st.info(f"**Ph∆∞∆°ng ph√°p x·ª≠ l√Ω d·ªØ li·ªáu:** {method_name}")
    
    # T·∫°o 3 c·ªôt cho 3 ti√™u ch√≠ ƒë√°nh gi√°
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    padding: 20px; border-radius: 10px; color: white; text-align: center;'>
            <h4 style='margin: 0; color: white;'>üèÜ T·ªïng th·ªÉ t·ªët nh·∫•t</h4>
            <p style='margin: 5px 0; font-size: 0.9em;'>D·ª±a tr√™n F1-Score</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown(f"**M√¥ h√¨nh:** {best_model.model_name}")
        st.metric("F1-Score", f"{best_model.metrics['f1_score']*100:.2f}%")
        
        col_a, col_b = st.columns(2)
        with col_a:
            st.metric("TP", f"{best_model.metrics['true_positive']:,}", 
                     help="True Positive - Gian l·∫≠n ph√°t hi·ªán ƒë√∫ng")
        with col_b:
            st.metric("FP", f"{best_model.metrics['false_positive']:,}",
                     help="False Positive - Nh·∫≠n di·ªán nh·∫ßm")
    
    with col2:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); 
                    padding: 20px; border-radius: 10px; color: white; text-align: center;'>
            <h4 style='margin: 0; color: white;'>üéØ Ph√°t hi·ªán t·ªët nh·∫•t</h4>
            <p style='margin: 5px 0; font-size: 0.9em;'>True Positive cao nh·∫•t</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown(f"**M√¥ h√¨nh:** {best_tp_model.model_name}")
        st.metric("True Positive", f"{best_tp_model.metrics['true_positive']:,}")
        
        col_c, col_d = st.columns(2)
        with col_c:
            st.metric("Recall", f"{best_tp_model.metrics['recall']*100:.2f}%",
                     help="T·ª∑ l·ªá ph√°t hi·ªán gian l·∫≠n")
        with col_d:
            st.metric("F1", f"{best_tp_model.metrics['f1_score']*100:.2f}%")
    
    with col3:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); 
                    padding: 20px; border-radius: 10px; color: white; text-align: center;'>
            <h4 style='margin: 0; color: white;'>‚ú® Ch√≠nh x√°c nh·∫•t</h4>
            <p style='margin: 5px 0; font-size: 0.9em;'>False Positive th·∫•p nh·∫•t</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown(f"**M√¥ h√¨nh:** {best_fp_model.model_name}")
        st.metric("False Positive", f"{best_fp_model.metrics['false_positive']:,}")
        
        col_e, col_f = st.columns(2)
        with col_e:
            st.metric("Precision", f"{best_fp_model.metrics['precision']*100:.2f}%",
                     help="ƒê·ªô ch√≠nh x√°c khi d·ª± ƒëo√°n gian l·∫≠n")
        with col_f:
            st.metric("F1", f"{best_fp_model.metrics['f1_score']*100:.2f}%")
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    # K·∫øt lu·∫≠n v·ªõi highlight
    conclusion_text = f"""
    <div style='background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%); 
                padding: 20px; border-radius: 10px; border-left: 5px solid #ff6b6b;'>
        <h4 style='margin-top: 0; color: #2d3436;'>üí° K·∫øt lu·∫≠n</h4>
        <p style='font-size: 1.05em; line-height: 1.6; color: #2d3436; margin-bottom: 0;'>
            M√¥ h√¨nh <strong style='color: #d63031;'>{best_model.model_name}</strong> k·∫øt h·ª£p v·ªõi 
            ph∆∞∆°ng ph√°p <strong style='color: #d63031;'>{method_name}</strong> cho ra k·∫øt qu·∫£ 
            <strong>c√¢n b·∫±ng t·ªët nh·∫•t</strong> gi·ªØa kh·∫£ nƒÉng ph√°t hi·ªán gian l·∫≠n (True Positive) 
            v√† t·ª∑ l·ªá nh·∫≠n di·ªán nh·∫ßm (False Positive).
        </p>
    </div>
    """
    
    st.markdown(conclusion_text, unsafe_allow_html=True)
    
    # Th√™m insights n·∫øu c√≥ s·ª± kh√°c bi·ªát gi·ªØa c√°c m√¥ h√¨nh
    if len(models) > 1:
        st.markdown("<br>", unsafe_allow_html=True)
        
        with st.expander("üìà Ph√¢n t√≠ch chi ti·∫øt v√† ƒë·ªÅ xu·∫•t"):
            if best_model.model_name != best_tp_model.model_name:
                st.warning(f"""
                **L∆∞u √Ω:** M√¥ h√¨nh **{best_tp_model.model_name}** ph√°t hi·ªán ƒë∆∞·ª£c nhi·ªÅu gian l·∫≠n h∆°n 
                ({best_tp_model.metrics['true_positive']} so v·ªõi {best_model.metrics['true_positive']}), 
                nh∆∞ng c√≥ th·ªÉ c√≥ nhi·ªÅu c·∫£nh b√°o gi·∫£ h∆°n ({best_tp_model.metrics['false_positive']} FP).
                
                **ƒê·ªÅ xu·∫•t:** N·∫øu ∆∞u ti√™n ph√°t hi·ªán t·ªëi ƒëa gian l·∫≠n v√† ch·∫•p nh·∫≠n m·ªôt s·ªë c·∫£nh b√°o gi·∫£, 
                h√£y xem x√©t s·ª≠ d·ª•ng **{best_tp_model.model_name}**.
                """)
            
            if best_model.model_name != best_fp_model.model_name:
                st.info(f"""
                **Ghi ch√∫:** M√¥ h√¨nh **{best_fp_model.model_name}** c√≥ s·ªë l∆∞·ª£ng c·∫£nh b√°o gi·∫£ th·∫•p nh·∫•t 
                ({best_fp_model.metrics['false_positive']} FP), ph√π h·ª£p n·∫øu c·∫ßn gi·∫£m thi·ªÉu phi·ªÅn h√† cho kh√°ch h√†ng.
                
                **ƒê·ªÅ xu·∫•t:** N·∫øu ∆∞u ti√™n tr·∫£i nghi·ªám kh√°ch h√†ng v√† gi·∫£m s·ªë l·∫ßn t·ª´ ch·ªëi nh·∫ßm giao d·ªãch h·ª£p ph√°p,
                h√£y xem x√©t **{best_fp_model.model_name}**.
                """)
            
            # So s√°nh performance
            f1_scores = [m.metrics['f1_score'] for m in models]
            f1_diff = (max(f1_scores) - min(f1_scores)) * 100
            
            if f1_diff < 1:
                st.success(f"""
                ‚úÖ **K·∫øt qu·∫£ ·ªïn ƒë·ªãnh:** C√°c m√¥ h√¨nh c√≥ hi·ªáu su·∫•t t∆∞∆°ng ƒë∆∞∆°ng nhau (ch√™nh l·ªách F1-Score < 1%). 
                C√≥ th·ªÉ ch·ªçn b·∫•t k·ª≥ m√¥ h√¨nh n√†o t√πy theo ti√™u ch√≠ ∆∞u ti√™n (t·ªëc ƒë·ªô, t√†i nguy√™n, kh·∫£ nƒÉng gi·∫£i th√≠ch).
                """)
            else:
                st.warning(f"""
                ‚ö†Ô∏è **Ch√™nh l·ªách ƒë√°ng k·ªÉ:** F1-Score ch√™nh l·ªách {f1_diff:.2f}% gi·ªØa c√°c m√¥ h√¨nh. 
                N√™n ch·ªçn m√¥ h√¨nh c√≥ hi·ªáu su·∫•t cao nh·∫•t cho production.
                """)


def create_detailed_metrics_table(models):
    """
    T·∫°o b·∫£ng metrics chi ti·∫øt v·ªõi highlight.
    
    Args:
        models (list): Danh s√°ch c√°c FraudDetectionModel
        
    Returns:
        pd.DataFrame: Styled DataFrame
    """
    df = create_metrics_dataframe(models)
    
    # H√†m ƒë·ªÉ highlight gi√° tr·ªã t·ªët nh·∫•t
    def highlight_best(s, props=''):
        # T√¨m index c·ªßa gi√° tr·ªã t·ªët nh·∫•t t√πy theo c·ªôt
        if s.name in ['True Positive (TP)', 'Accuracy (%)', 'Precision (%)', 'Recall (%)', 'F1-Score (%)']:
            # V·ªõi c√°c metrics n√†y, gi√° tr·ªã cao h∆°n l√† t·ªët h∆°n
            # Chuy·ªÉn v·ªÅ s·ªë ƒë·ªÉ so s√°nh
            numeric_values = s.str.rstrip('%').astype(float) if s.dtype == 'object' else s
            best_idx = numeric_values.idxmax()
        elif s.name == 'False Positive (FP)':
            # V·ªõi FP, gi√° tr·ªã th·∫•p h∆°n l√† t·ªët h∆°n
            best_idx = s.idxmin()
        else:
            return [''] * len(s)
        
        return ['background-color: lightgreen; font-weight: bold' if i == best_idx else '' for i in range(len(s))]
    
    # √Åp d·ª•ng styling v√†o DataFrame
    styled_df = df.style.apply(highlight_best, axis=0)
    
    return styled_df
