# ðŸ“š TÃ€I LIá»†U Ká»¸ THUáº¬T - PHÃT HIá»†N GIAN Láº¬N THáºº TÃN Dá»¤NG

> **Äá»“ Ã¡n**: PhÃ¡t hiá»‡n Gian láº­n Tháº» TÃ­n dá»¥ng báº±ng Machine Learning  
> **Tá»« Research Paper â†’ Production Demo**  
> **NgÃ y cáº­p nháº­t**: 31/10/2025

---

## ðŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng quan kiáº¿n trÃºc](#1-tá»•ng-quan-kiáº¿n-trÃºc)
2. [LÃ½ thuyáº¿t Machine Learning](#2-lÃ½-thuyáº¿t-machine-learning)
3. [Xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng](#3-xá»­-lÃ½-dá»¯-liá»‡u-máº¥t-cÃ¢n-báº±ng)
4. [CÃ¡c mÃ´ hÃ¬nh ML triá»ƒn khai](#4-cÃ¡c-mÃ´-hÃ¬nh-ml-triá»ƒn-khai)
5. [Pipeline xá»­ lÃ½ dá»¯ liá»‡u](#5-pipeline-xá»­-lÃ½-dá»¯-liá»‡u)
6. [ÄÃ¡nh giÃ¡ vÃ  Metrics](#6-Ä‘Ã¡nh-giÃ¡-vÃ -metrics)
7. [Tá»« Training Ä‘áº¿n Production](#7-tá»«-training-Ä‘áº¿n-production)
8. [Best Practices](#8-best-practices)

---

## 1. Tá»”NG QUAN KIáº¾N TRÃšC

### 1.1. Kiáº¿n trÃºc tá»•ng thá»ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRAUD DETECTION SYSTEM                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PRESENTATION LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   app.py - Streamlit Web Application               â”‚     â”‚
â”‚  â”‚   - Sidebar Controls                                â”‚     â”‚
â”‚  â”‚   - Main Display Area                               â”‚     â”‚
â”‚  â”‚   - Interactive Visualizations                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BUSINESS LOGIC LAYER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚    models.py â”‚  â”‚data_processingâ”‚  â”‚visualization â”‚      â”‚
â”‚  â”‚              â”‚  â”‚      .py      â”‚  â”‚     .py      â”‚      â”‚
â”‚  â”‚  - Logistic  â”‚  â”‚  - Load Data  â”‚  â”‚  - Confusion â”‚      â”‚
â”‚  â”‚  - D.Tree    â”‚  â”‚  - Prepare    â”‚  â”‚    Matrix    â”‚      â”‚
â”‚  â”‚  - Bayesian  â”‚  â”‚  - Oversample â”‚  â”‚  - Charts    â”‚      â”‚
â”‚  â”‚  - Train     â”‚  â”‚  - SMOTE      â”‚  â”‚  - Tables    â”‚      â”‚
â”‚  â”‚  - Evaluate  â”‚  â”‚  - Split      â”‚  â”‚  - Recommend â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   data/creditcard.csv (284,807 transactions)       â”‚     â”‚
â”‚  â”‚   - 30 features (Time, Amount, V1-V28)             â”‚     â”‚
â”‚  â”‚   - Binary target (Class: 0/1)                     â”‚     â”‚
â”‚  â”‚   - Highly imbalanced (0.172% fraud)               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2. Luá»“ng xá»­ lÃ½ dá»¯ liá»‡u (Data Flow)

```
ðŸ“Š RAW DATA                                    ðŸŽ¯ PREDICTIONS
    â”‚
    â”œâ”€â–º Load CSV (creditcard.csv)
    â”‚       â”‚
    â”‚       â–¼
    â”œâ”€â–º Preprocess (prepare_data)
    â”‚       â”‚
    â”‚       â”œâ”€â–º Normalize Amount (StandardScaler)
    â”‚       â”œâ”€â–º Drop Time column
    â”‚       â”œâ”€â–º Split features/target (X, y)
    â”‚       â””â”€â–º Train/Test Split (80/20)
    â”‚               â”‚
    â”‚               â–¼
    â”œâ”€â–º Resample Training Data
    â”‚       â”‚
    â”‚       â”œâ”€â–º Option 1: Original (Imbalanced)
    â”‚       â”œâ”€â–º Option 2: Random Oversampling
    â”‚       â””â”€â–º Option 3: SMOTE
    â”‚               â”‚
    â”‚               â–¼
    â”œâ”€â–º Train Models
    â”‚       â”‚
    â”‚       â”œâ”€â–º Logistic Regression
    â”‚       â”œâ”€â–º Decision Tree
    â”‚       â””â”€â–º Bayesian Network
    â”‚               â”‚
    â”‚               â–¼
    â”œâ”€â–º Predict on Test Set
    â”‚       â”‚
    â”‚       â–¼
    â””â”€â–º Evaluate Performance
            â”‚
            â”œâ”€â–º Confusion Matrix
            â”œâ”€â–º Accuracy, Precision, Recall
            â”œâ”€â–º F1-Score
            â””â”€â–º Recommendations
```

---

## 2. LÃ THUYáº¾T MACHINE LEARNING

### 2.1. BÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (Binary Classification)

**Äá»‹nh nghÄ©a:**
- Input: Vector Ä‘áº·c trÆ°ng X = [Amount, V1, V2, ..., V28] âˆˆ â„Â²â¹
- Output: Class y âˆˆ {0, 1} (0 = Legitimate, 1 = Fraud)
- Má»¥c tiÃªu: TÃ¬m hÃ m f: â„Â²â¹ â†’ {0, 1}

**ThÃ¡ch thá»©c:**
1. **Imbalanced Data**: Fraud cases chá»‰ chiáº¿m 0.172%
2. **Cost-sensitive**: FN (bá» sÃ³t gian láº­n) nghiÃªm trá»ng hÆ¡n FP
3. **Dimensionality**: 29 features sau PCA

### 2.2. Supervised Learning Pipeline

```python
# 1. Data Preparation
X_train, X_test, y_train, y_test = train_test_split(X, y)

# 2. Resampling (náº¿u cáº§n)
X_resampled, y_resampled = apply_smote(X_train, y_train)

# 3. Model Training
model.fit(X_resampled, y_resampled)

# 4. Prediction
y_pred = model.predict(X_test)

# 5. Evaluation
metrics = evaluate(y_test, y_pred)
```

### 2.3. Bias-Variance Tradeoff

```
High Bias (Underfitting)     â†â†’     High Variance (Overfitting)
        â”‚                                      â”‚
        â”‚                                      â”‚
  Logistic Regression                   Deep Decision Tree
        â”‚                                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                  Sweet Spot
                 (Best Model)
```

**Trong dá»± Ã¡n:**
- Logistic Regression: High bias, low variance
- Decision Tree: Low bias, high variance (cáº§n tuning)
- Bayesian: Moderate bias-variance

---

## 3. Xá»¬ LÃ Dá»® LIá»†U Máº¤T CÃ‚N Báº°NG

### 3.1. Váº¥n Ä‘á» Imbalanced Data

**Dataset gá»‘c:**
```
Class 0 (Legitimate): 284,315 samples (99.828%)
Class 1 (Fraud):          492 samples ( 0.172%)
Imbalance Ratio: 578:1
```

**Háº­u quáº£:**
- Model há»c thiÃªn vá»‹ vá» majority class
- Precision cao nhÆ°ng Recall tháº¥p (bá» sÃ³t gian láº­n)
- Accuracy khÃ´ng pháº£n Ã¡nh hiá»‡u suáº¥t thá»±c táº¿

### 3.2. PhÆ°Æ¡ng phÃ¡p 1: Random Oversampling

**NguyÃªn lÃ½:**
```python
# Láº·p láº¡i ngáº«u nhiÃªn cÃ¡c máº«u minority class
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

# Káº¿t quáº£: Class 0 = Class 1 (cÃ¢n báº±ng 50-50)
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ¡n giáº£n, dá»… implement
- âœ… Nhanh, khÃ´ng tá»‘n tÃ i nguyÃªn
- âœ… KhÃ´ng thay Ä‘á»•i phÃ¢n phá»‘i dá»¯ liá»‡u gá»‘c

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Overfitting (láº·p láº¡i chÃ­nh xÃ¡c cÃ¹ng máº«u)
- âŒ KhÃ´ng táº¡o thÃ´ng tin má»›i
- âŒ CÃ³ thá»ƒ há»c "nhiá»…u" tá»« cÃ¡c máº«u outlier

**Code implementation:**
```python
def apply_oversampling(X_train, y_train, random_state=42):
    """
    Ãp dá»¥ng Random Oversampling Ä‘á»ƒ cÃ¢n báº±ng dá»¯ liá»‡u.
    
    PhÆ°Æ¡ng phÃ¡p: Láº·p láº¡i cÃ¡c máº«u minority class ngáº«u nhiÃªn
    cho Ä‘áº¿n khi cÃ¢n báº±ng vá»›i majority class.
    """
    ros = RandomOverSampler(random_state=random_state)
    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)
    
    return X_resampled, y_resampled
```

### 3.3. PhÆ°Æ¡ng phÃ¡p 2: SMOTE

**NguyÃªn lÃ½:**
```
SMOTE (Synthetic Minority Oversampling Technique)

1. Chá»n 1 máº«u minority class: x_i
2. TÃ¬m k nearest neighbors (thÆ°á»ng k=5)
3. Chá»n ngáº«u nhiÃªn 1 neighbor: x_nn
4. Táº¡o máº«u má»›i: x_new = x_i + Î» Ã— (x_nn - x_i)
   vá»›i Î» âˆˆ [0, 1] ngáº«u nhiÃªn
5. Láº·p láº¡i cho Ä‘áº¿n khi cÃ¢n báº±ng
```

**Minh há»a:**
```
     x_nn
      â—
      â”‚â•²
      â”‚ â•² x_new (synthetic)
      â”‚  âŠ—
      â”‚ â•±
      â”‚â•±
      â—
     x_i
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Táº¡o dá»¯ liá»‡u Ä‘a dáº¡ng (synthetic samples)
- âœ… Giáº£m overfitting
- âœ… Cáº£i thiá»‡n generalization
- âœ… Há»c Ä‘Æ°á»£c vÃ¹ng quyáº¿t Ä‘á»‹nh tá»‘t hÆ¡n

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Tá»‘n thá»i gian tÃ­nh toÃ¡n (k-NN)
- âŒ CÃ³ thá»ƒ táº¡o máº«u "khÃ´ng há»£p lÃ½" náº¿u dá»¯ liá»‡u nhiá»…u
- âŒ KhÃ´ng hiá»‡u quáº£ vá»›i high-dimensional data

**Code implementation:**
```python
def apply_smote(X_train, y_train, random_state=42):
    """
    Ãp dá»¥ng SMOTE Ä‘á»ƒ táº¡o máº«u synthetic minority class.
    
    PhÆ°Æ¡ng phÃ¡p: Ná»™i suy giá»¯a cÃ¡c máº«u minority class vÃ 
    k-nearest neighbors cá»§a chÃºng Ä‘á»ƒ táº¡o máº«u má»›i.
    """
    smote = SMOTE(random_state=random_state)
    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
    
    return X_resampled, y_resampled
```

### 3.4. So sÃ¡nh Oversampling vs SMOTE

| Aspect | Random Oversampling | SMOTE |
|--------|-------------------|-------|
| **Tá»‘c Ä‘á»™** | Ráº¥t nhanh | Cháº­m hÆ¡n (k-NN) |
| **Overfitting** | Cao | Tháº¥p hÆ¡n |
| **Diversity** | KhÃ´ng cÃ³ | Cao |
| **Memory** | Ãt | Nhiá»u hÆ¡n |
| **Best for** | Quick prototype | Production model |
| **Performance** | Tá»‘t | Tá»‘t hÆ¡n thÆ°á»ng xuyÃªn |

---

## 4. CÃC MÃ” HÃŒNH ML TRIá»‚N KHAI

### 4.1. Logistic Regression

**LÃ½ thuyáº¿t:**
```
HÃ m dá»± Ä‘oÃ¡n:
    Å· = sigmoid(w^T x + b)
    
Sigmoid function:
    Ïƒ(z) = 1 / (1 + e^(-z))
    
Loss function (Binary Cross-Entropy):
    L(w,b) = -1/m Î£[y log(Å·) + (1-y)log(1-Å·)]
    
Optimization:
    Gradient Descent hoáº·c L-BFGS
```

**Äáº·c Ä‘iá»ƒm:**
- **Tuyáº¿n tÃ­nh**: Decision boundary lÃ  siÃªu pháº³ng
- **XÃ¡c suáº¥t**: Output lÃ  xÃ¡c suáº¥t thuá»™c class 1
- **Fast training**: Converge nhanh vá»›i dá»¯ liá»‡u lá»›n
- **Interpretable**: Weights thá»ƒ hiá»‡n táº§m quan trá»ng features

**Implementation:**
```python
class LogisticRegressionModel(FraudDetectionModel):
    """
    MÃ´ hÃ¬nh Há»“i quy Logistic cho phÃ¢n loáº¡i nhá»‹ phÃ¢n.
    
    Sá»­ dá»¥ng hÃ m sigmoid Ä‘á»ƒ map linear combination cá»§a
    features thÃ nh xÃ¡c suáº¥t [0,1].
    """
    
    def __init__(self, max_iter=1000, random_state=42):
        model = LogisticRegression(
            max_iter=max_iter,      # Sá»‘ vÃ²ng láº·p tá»‘i Ä‘a
            random_state=random_state,
            n_jobs=-1,              # Parallel processing
            solver='lbfgs'          # Optimization algorithm
        )
        super().__init__('Há»“i quy Logistic', model)
```

**Khi nÃ o sá»­ dá»¥ng:**
- âœ… Baseline model (always start here)
- âœ… Cáº§n interpretability
- âœ… Dá»¯ liá»‡u lá»›n, cáº§n training nhanh
- âœ… Features cÃ³ quan há»‡ gáº§n tuyáº¿n tÃ­nh vá»›i target

### 4.2. Decision Tree

**LÃ½ thuyáº¿t:**
```
Cáº¥u trÃºc cÃ¢y:
                [Amount > 100?]
                /            \
              Yes             No
              /                \
      [V1 > 0.5?]         [V2 > 0.3?]
       /      \             /      \
    Fraud   Normal      Fraud   Normal
    
Splitting Criterion (Gini Impurity):
    Gini(p) = 1 - Î£(p_iÂ²)
    
Information Gain:
    IG = Gini(parent) - Î£(weighted_Gini(children))
```

**Äáº·c Ä‘iá»ƒm:**
- **Phi tuyáº¿n**: CÃ³ thá»ƒ há»c decision boundary phá»©c táº¡p
- **Non-parametric**: KhÃ´ng giáº£ Ä‘á»‹nh vá» phÃ¢n phá»‘i dá»¯ liá»‡u
- **Feature importance**: Tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ features
- **Overfitting prone**: Cáº§n pruning (max_depth)

**Implementation:**
```python
class DecisionTreeModel(FraudDetectionModel):
    """
    MÃ´ hÃ¬nh CÃ¢y quyáº¿t Ä‘á»‹nh.
    
    Táº¡o cáº¥u trÃºc cÃ¢y phÃ¢n loáº¡i báº±ng cÃ¡ch chia khÃ´ng gian
    features dá»±a trÃªn tiÃªu chÃ­ Gini impurity.
    """
    
    def __init__(self, max_depth=10, random_state=42):
        model = DecisionTreeClassifier(
            max_depth=max_depth,        # Giá»›i háº¡n Ä‘á»™ sÃ¢u Ä‘á»ƒ trÃ¡nh overfitting
            random_state=random_state,
            criterion='gini',           # Splitting criterion
            min_samples_split=2,        # Minimum samples to split
            min_samples_leaf=1          # Minimum samples in leaf
        )
        super().__init__('CÃ¢y quyáº¿t Ä‘á»‹nh', model)
```

**Khi nÃ o sá»­ dá»¥ng:**
- âœ… Dá»¯ liá»‡u cÃ³ quan há»‡ phi tuyáº¿n
- âœ… Cáº§n interpretability (visualize tree)
- âœ… Features cÃ³ interactions
- âš ï¸ Cáº§n tuning hyperparameters cáº©n tháº­n

### 4.3. Bayesian Network (Gaussian Naive Bayes)

**LÃ½ thuyáº¿t:**
```
Bayes' Theorem:
    P(Fraud|X) = P(X|Fraud) Ã— P(Fraud) / P(X)
    
Naive Assumption (Independence):
    P(X|Fraud) = P(xâ‚|Fraud) Ã— P(xâ‚‚|Fraud) Ã— ... Ã— P(xâ‚™|Fraud)
    
Gaussian Distribution (cho continuous features):
    P(xáµ¢|Fraud) = 1/âˆš(2Ï€ÏƒÂ²) Ã— e^(-(xáµ¢-Î¼)Â²/(2ÏƒÂ²))
    
Decision Rule:
    Å· = argmax_c P(c) Ã— âˆ P(xáµ¢|c)
```

**Äáº·c Ä‘iá»ƒm:**
- **Probabilistic**: Dá»±a trÃªn lÃ½ thuyáº¿t xÃ¡c suáº¥t
- **Fast**: Training vÃ  prediction ráº¥t nhanh
- **Small data**: Hoáº¡t Ä‘á»™ng tá»‘t vá»›i Ã­t dá»¯ liá»‡u
- **Independence assumption**: Giáº£ Ä‘á»‹nh features Ä‘á»™c láº­p (naive)

**Implementation:**
```python
class BayesianNetworkModel(FraudDetectionModel):
    """
    MÃ´ hÃ¬nh Máº¡ng Bayesian (Gaussian Naive Bayes).
    
    Sá»­ dá»¥ng Ä‘á»‹nh lÃ½ Bayes vá»›i giáº£ Ä‘á»‹nh cÃ¡c features
    Ä‘á»™c láº­p cÃ³ Ä‘iá»u kiá»‡n (conditional independence).
    """
    
    def __init__(self):
        model = GaussianNB()
        # KhÃ´ng cáº§n hyperparameters cho Gaussian NB
        super().__init__('Máº¡ng Bayesian', model)
```

**Khi nÃ o sá»­ dá»¥ng:**
- âœ… Dá»¯ liá»‡u nhá»
- âœ… Cáº§n training/prediction cá»±c nhanh
- âœ… Features gáº§n nhÆ° Ä‘á»™c láº­p
- âœ… Baseline probabilistic model

### 4.4. So sÃ¡nh cÃ¡c mÃ´ hÃ¬nh

| Model | Complexity | Speed | Overfitting | Interpretability |
|-------|-----------|-------|-------------|------------------|
| **Logistic** | Low | Fast | Low | High |
| **Decision Tree** | Medium-High | Medium | High | High |
| **Bayesian** | Low | Very Fast | Low | Medium |

---

## 5. PIPELINE Xá»¬ LÃ Dá»® LIá»†U

### 5.1. Data Preprocessing (theo bÃ i bÃ¡o)

**Quy trÃ¬nh chuáº©n:**
```python
def prepare_data(df, test_size=0.2, random_state=42):
    """
    Pipeline xá»­ lÃ½ dá»¯ liá»‡u theo Ä‘Ãºng bÃ i bÃ¡o:
    
    1. Normalize Amount (chá»‰ Amount, khÃ´ng pháº£i V1-V28)
    2. Drop Time column
    3. Split features and target
    4. Train/Test split (stratified)
    """
    # BÆ°á»›c 1: Copy Ä‘á»ƒ khÃ´ng áº£nh hÆ°á»Ÿng dá»¯ liá»‡u gá»‘c
    df_processed = df.copy()
    
    # BÆ°á»›c 2: Chuáº©n hÃ³a Amount (StandardScaler)
    # LÃ½ do: Amount cÃ³ range ráº¥t lá»›n, cáº§n normalize
    # V1-V28 Ä‘Ã£ Ä‘Æ°á»£c PCA nÃªn KHÃ”NG cáº§n normalize láº¡i
    scaler = StandardScaler()
    df_processed['Amount'] = scaler.fit_transform(
        df_processed['Amount'].values.reshape(-1, 1)
    )
    
    # BÆ°á»›c 3: Loáº¡i bá» Time
    # LÃ½ do: Time khÃ´ng cÃ³ Ã½ nghÄ©a prediction trong context nÃ y
    df_processed = df_processed.drop('Time', axis=1)
    
    # BÆ°á»›c 4: TÃ¡ch features vÃ  target
    X = df_processed.drop('Class', axis=1)  # 29 features
    y = df_processed['Class']                # Binary target
    
    # BÆ°á»›c 5: Chia train/test vá»›i stratify
    # stratify=y Ä‘áº£m báº£o tá»· lá»‡ fraud giá»‘ng nhau trong train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=test_size,      # 20% test
        random_state=random_state,
        stratify=y                # Quan trá»ng vá»›i imbalanced data
    )
    
    # Chuyá»ƒn vá» numpy array
    X_train = X_train.values
    X_test = X_test.values
    
    return X_train, X_test, y_train, y_test
```

**LÆ°u Ã½ quan trá»ng:**
1. âŒ **KHÃ”NG** chuáº©n hÃ³a V1-V28 (Ä‘Ã£ Ä‘Æ°á»£c PCA)
2. âœ… **CHá»ˆ** chuáº©n hÃ³a Amount
3. âœ… Loáº¡i bá» Time **TRÆ¯á»šC** khi split
4. âœ… Chuáº©n hÃ³a **TRÆ¯á»šC** khi split (trÃ¡nh data leakage)
5. âœ… Sá»­ dá»¥ng stratified split

### 5.2. Resampling Strategy

**Factory Pattern:**
```python
def process_data_by_method(method, X_train, y_train):
    """
    Xá»­ lÃ½ dá»¯ liá»‡u theo phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c chá»n.
    
    Design Pattern: Strategy Pattern
    - Encapsulates resampling algorithms
    - Allows runtime selection
    """
    if method == 'Dá»¯ liá»‡u gá»‘c (Máº¥t cÃ¢n báº±ng)':
        # KhÃ´ng resample
        method_info = get_resampling_info(y_train, y_train, 'Original')
        return X_train, y_train, method_info
        
    elif method == 'Xá»­ lÃ½ báº±ng Oversampling':
        X_resampled, y_resampled = apply_oversampling(X_train, y_train)
        method_info = get_resampling_info(y_train, y_resampled, 'Oversampling')
        return X_resampled, y_resampled, method_info
        
    elif method == 'Xá»­ lÃ½ báº±ng SMOTE':
        X_resampled, y_resampled = apply_smote(X_train, y_train)
        method_info = get_resampling_info(y_train, y_resampled, 'SMOTE')
        return X_resampled, y_resampled, method_info
```

### 5.3. Train-Test Split Strategy

**Stratified Splitting:**
```
Original Distribution:
    Fraud: 0.172%
    
Train Set (80%):
    Fraud: 0.172% (same ratio maintained)
    
Test Set (20%):
    Fraud: 0.172% (same ratio maintained)
    
âœ… Benefits:
    - Representative samples
    - Consistent evaluation
    - No bias in split
```

---

## 6. ÄÃNH GIÃ VÃ€ METRICS

### 6.1. Confusion Matrix

**Äá»‹nh nghÄ©a:**
```
                    Predicted
                  Negative  Positive
                  (Normal)  (Fraud)
Actual  Negative    TN        FP
        (Normal)
        
        Positive    FN        TP
        (Fraud)
```

**Ã nghÄ©a trong fraud detection:**
- **TN (True Negative)**: âœ… Normal transaction predicted correctly
- **TP (True Positive)**: âœ… Fraud transaction detected correctly
- **FP (False Positive)**: âŒ Normal flagged as fraud (False alarm)
- **FN (False Negative)**: âŒ Fraud missed (Serious!)

**Code implementation:**
```python
def evaluate(self, y_test, y_pred=None):
    """
    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i confusion matrix vÃ  metrics.
    """
    if y_pred is None:
        y_pred = self.predictions
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    
    return {
        'confusion_matrix': cm,
        'true_negative': int(tn),
        'false_positive': int(fp),
        'false_negative': int(fn),
        'true_positive': int(tp),
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1)
    }
```

### 6.2. Metrics Chi tiáº¿t

**Accuracy:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

Æ¯u Ä‘iá»ƒm: Dá»… hiá»ƒu
NhÆ°á»£c Ä‘iá»ƒm: KHÃ”NG phÃ¹ há»£p vá»›i imbalanced data
    â†’ Model predict táº¥t cáº£ lÃ  Normal â†’ 99.8% accuracy!
```

**Precision:**
```
Precision = TP / (TP + FP)

Ã nghÄ©a: Trong sá»‘ cÃ¡c giao dá»‹ch Ä‘Æ°á»£c dá»± Ä‘oÃ¡n lÃ  FRAUD,
         bao nhiÃªu % thá»±c sá»± lÃ  FRAUD?
         
Impact: FP cao â†’ Nhiá»u false alarms â†’ KhÃ¡ch hÃ ng khÃ³ chá»‹u
```

**Recall (Sensitivity, True Positive Rate):**
```
Recall = TP / (TP + FN)

Ã nghÄ©a: Trong sá»‘ cÃ¡c giao dá»‹ch THá»°C Sá»° LÃ€ FRAUD,
         bao nhiÃªu % Ä‘Æ°á»£c phÃ¡t hiá»‡n?
         
Impact: FN cao â†’ Bá» sÃ³t gian láº­n â†’ Loss tiá»n
Priority: RECALL cao lÃ  quan trá»ng nháº¥t trong fraud detection!
```

**F1-Score:**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

Ã nghÄ©a: Harmonic mean cá»§a Precision vÃ  Recall
         CÃ¢n báº±ng giá»¯a 2 metrics
         
Best for: Imbalanced classification evaluation
```

### 6.3. Precision-Recall Tradeoff

```
High Threshold (0.9):
    â†’ Few predictions as Fraud
    â†’ High Precision, Low Recall
    â†’ Miss many frauds (bad!)
    
Low Threshold (0.1):
    â†’ Many predictions as Fraud
    â†’ Low Precision, High Recall
    â†’ Many false alarms (annoying but safer)
    
Optimal Point:
    â†’ Depends on business requirements
    â†’ Cost of FN vs Cost of FP
```

### 6.4. Metric Selection trong Fraud Detection

**Priority ranking:**
1. **Recall** (Most important) - KhÃ´ng bá» sÃ³t gian láº­n
2. **F1-Score** - CÃ¢n báº±ng tá»•ng thá»ƒ
3. **Precision** - Giáº£m false alarms
4. **Accuracy** (Least important) - Misleading vá»›i imbalanced data

---

## 7. Tá»ª TRAINING Äáº¾N PRODUCTION

### 7.1. Model Training Flow

```python
def train_and_evaluate_models(model_names, X_train, y_train, X_test, y_test):
    """
    Pipeline hoÃ n chá»‰nh tá»« training Ä‘áº¿n evaluation.
    
    Flow:
    1. Iterate through selected models
    2. Create model instance (Factory Pattern)
    3. Train on resampled data
    4. Predict on original test set
    5. Evaluate and collect metrics
    6. Return trained models with results
    """
    trained_models = []
    
    for model_name in model_names:
        # Step 1: Create model
        model = create_model(model_name)
        
        # Step 2: Train
        model.train(X_train, y_train)
        
        # Step 3: Predict
        model.predict(X_test)
        
        # Step 4: Evaluate
        model.evaluate(y_test)
        
        # Step 5: Collect
        trained_models.append(model)
    
    return trained_models
```

### 7.2. Production Demo Architecture

**Component Structure:**
```
app.py (Main Application)
â”œâ”€â”€ Presentation Layer
â”‚   â”œâ”€â”€ Streamlit UI components
â”‚   â”œâ”€â”€ Sidebar controls
â”‚   â”œâ”€â”€ Main display area
â”‚   â””â”€â”€ Interactive visualizations
â”‚
â”œâ”€â”€ Business Logic Layer
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”‚   â”œâ”€â”€ load_data()
â”‚   â”‚   â”œâ”€â”€ prepare_data()
â”‚   â”‚   â”œâ”€â”€ apply_oversampling()
â”‚   â”‚   â”œâ”€â”€ apply_smote()
â”‚   â”‚   â””â”€â”€ process_data_by_method()
â”‚   â”‚
â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ FraudDetectionModel (Base Class)
â”‚   â”‚   â”œâ”€â”€ LogisticRegressionModel
â”‚   â”‚   â”œâ”€â”€ DecisionTreeModel
â”‚   â”‚   â”œâ”€â”€ BayesianNetworkModel
â”‚   â”‚   â”œâ”€â”€ create_model() (Factory)
â”‚   â”‚   â””â”€â”€ train_and_evaluate_models()
â”‚   â”‚
â”‚   â””â”€â”€ visualization.py
â”‚       â”œâ”€â”€ plot_confusion_matrix()
â”‚       â”œâ”€â”€ create_metrics_dataframe()
â”‚       â”œâ”€â”€ display_metrics_summary()
â”‚       â”œâ”€â”€ plot_comparison_chart()
â”‚       â””â”€â”€ get_recommendation()
â”‚
â””â”€â”€ Data Layer
    â””â”€â”€ data/creditcard.csv
```

### 7.3. Design Patterns Used

**1. Factory Pattern:**
```python
def create_model(model_name):
    """
    Factory Ä‘á»ƒ táº¡o model instances.
    
    Benefits:
    - Centralized creation logic
    - Easy to extend with new models
    - Decoupling from concrete classes
    """
    models = {
        'Há»“i quy Logistic': LogisticRegressionModel,
        'CÃ¢y quyáº¿t Ä‘á»‹nh': DecisionTreeModel,
        'Máº¡ng Bayesian': BayesianNetworkModel,
    }
    
    if model_name not in models:
        raise ValueError(f"MÃ´ hÃ¬nh '{model_name}' khÃ´ng Ä‘Æ°á»£c há»— trá»£!")
    
    return models[model_name]()
```

**2. Strategy Pattern:**
```python
# Encapsulate resampling algorithms
strategies = {
    'original': lambda X, y: (X, y),
    'oversampling': apply_oversampling,
    'smote': apply_smote
}

# Runtime selection
X_resampled, y_resampled = strategies[method](X_train, y_train)
```

**3. Template Method Pattern:**
```python
class FraudDetectionModel:
    """
    Base class defining template for all models.
    """
    def train(self, X, y):
        # Common training logic
        pass
    
    def predict(self, X):
        # Common prediction logic
        pass
    
    def evaluate(self, y_test):
        # Common evaluation logic
        pass
```

### 7.4. Streamlit Integration

**Caching Strategy:**
```python
@st.cache_data
def load_data():
    """
    Cache data loading Ä‘á»ƒ trÃ¡nh reload nhiá»u láº§n.
    
    Benefits:
    - Faster app performance
    - Reduce API calls
    - Better UX
    """
    df = pd.read_csv(file_path)
    return df
```

**Interactive Controls:**
```python
# Sidebar controls
data_method = st.sidebar.selectbox(
    "PhÆ°Æ¡ng phÃ¡p xá»­ lÃ½:",
    ['Dá»¯ liá»‡u gá»‘c', 'Oversampling', 'SMOTE']
)

selected_models = st.sidebar.multiselect(
    "Chá»n mÃ´ hÃ¬nh:",
    ['Há»“i quy Logistic', 'CÃ¢y quyáº¿t Ä‘á»‹nh', 'Máº¡ng Bayesian']
)

# Action button
if st.sidebar.button("Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡"):
    # Execute pipeline
    run_training_pipeline()
```

**Visualization Display:**
```python
# Confusion Matrix
fig_cm = plot_confusion_matrix(cm, model_name)
st.pyplot(fig_cm)

# Metrics cards
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("True Positive", tp)
with col2:
    st.metric("False Positive", fp)
# ...

# Comparison charts
fig_comparison = plot_comparison_chart(models, 'f1_score')
st.pyplot(fig_comparison)
```

### 7.5. Error Handling & Validation

**Data Validation:**
```python
def load_data():
    try:
        df = pd.read_csv(file_path)
        
        # Validate structure
        required_columns = ['Time', 'Amount', 'Class']
        assert all(col in df.columns for col in required_columns)
        
        # Validate data types
        assert df['Class'].isin([0, 1]).all()
        
        return df
        
    except FileNotFoundError:
        st.error("File not found!")
        return None
    except Exception as e:
        st.error(f"Error: {e}")
        return None
```

**Model Validation:**
```python
def train(self, X_train, y_train):
    # Validate inputs
    if X_train.shape[0] != len(y_train):
        raise ValueError("Mismatch between X and y")
    
    # Training with progress
    with st.spinner(f'Training {self.model_name}...'):
        self.model.fit(X_train, y_train)
        self.is_trained = True
```

---

## 8. BEST PRACTICES

### 8.1. Data Science Best Practices

**1. Always Split BEFORE Resampling:**
```python
# âœ… CORRECT
X_train, X_test, y_train, y_test = train_test_split(X, y)
X_train_resampled, y_train_resampled = apply_smote(X_train, y_train)

# âŒ WRONG - Data leakage!
X_resampled, y_resampled = apply_smote(X, y)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled)
```

**2. Use Stratified Split:**
```python
# âœ… CORRECT - Maintains class distribution
train_test_split(X, y, stratify=y)

# âŒ WRONG - Random split may create bias
train_test_split(X, y)
```

**3. Normalize Before Split (if applying to all data):**
```python
# âœ… CORRECT - Normalize Amount before split
df['Amount'] = scaler.fit_transform(df['Amount'])
X_train, X_test = train_test_split(df)

# Note: Náº¿u normalize sau split, cáº§n fit trÃªn train, transform trÃªn test
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```

**4. Don't Normalize PCA Features:**
```python
# âœ… CORRECT
df['Amount'] = scaler.fit_transform(df['Amount'])
# V1-V28 khÃ´ng normalize

# âŒ WRONG
scaler.fit_transform(df[['Amount', 'V1', 'V2', ..., 'V28']])
```

### 8.2. Code Organization

**Module Structure:**
```
src/
â”œâ”€â”€ __init__.py          # Package initialization
â”œâ”€â”€ data_processing.py   # Single responsibility: Data
â”œâ”€â”€ models.py            # Single responsibility: Models
â””â”€â”€ visualization.py     # Single responsibility: Viz
```

**Class Design:**
```python
# âœ… GOOD - Base class with inheritance
class FraudDetectionModel:
    def train(self, X, y): pass
    def predict(self, X): pass
    def evaluate(self, y_test): pass

class LogisticRegressionModel(FraudDetectionModel):
    # Specific implementation
```

**Function Design:**
```python
# âœ… GOOD - Single purpose, clear naming
def apply_smote(X_train, y_train, random_state=42):
    """Clear docstring"""
    smote = SMOTE(random_state=random_state)
    return smote.fit_resample(X_train, y_train)

# âŒ BAD - Multiple responsibilities
def process_everything(df):
    # Load, clean, split, resample, train...
    pass
```

### 8.3. Performance Optimization

**1. Caching:**
```python
@st.cache_data  # Cache expensive operations
def load_data():
    return pd.read_csv(large_file)
```

**2. Parallel Processing:**
```python
LogisticRegression(n_jobs=-1)  # Use all CPU cores
```

**3. Memory Management:**
```python
# Convert to numpy when needed
X_train = X_train.values  # DataFrame â†’ numpy (faster)
```

### 8.4. Documentation

**1. Docstrings:**
```python
def apply_smote(X_train, y_train, random_state=42):
    """
    Ãp dá»¥ng SMOTE Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u máº¥t cÃ¢n báº±ng.
    
    Args:
        X_train (np.array): Training features
        y_train (pd.Series): Training labels
        random_state (int): Seed for reproducibility
        
    Returns:
        tuple: (X_resampled, y_resampled)
        
    Example:
        >>> X_res, y_res = apply_smote(X_train, y_train)
        >>> print(y_res.value_counts())
    """
```

**2. Comments:**
```python
# Good: Explain WHY, not WHAT
# Normalize Amount because it has large variance
# V1-V28 already normalized by PCA, don't touch

# Bad: State the obvious
# Loop through models
for model in models:
```

### 8.5. Testing Strategy

**Unit Tests:**
```python
def test_prepare_data():
    df = load_data()
    X_train, X_test, y_train, y_test = prepare_data(df)
    
    # Test split ratio
    assert len(X_test) / len(df) â‰ˆ 0.2
    
    # Test stratification
    assert y_train.mean() â‰ˆ y_test.mean()
    
    # Test features
    assert X_train.shape[1] == 29  # Time dropped
```

**Integration Tests:**
```python
def test_full_pipeline():
    df = load_data()
    X_train, X_test, y_train, y_test = prepare_data(df)
    X_res, y_res = apply_smote(X_train, y_train)
    
    model = LogisticRegressionModel()
    model.train(X_res, y_res)
    predictions = model.predict(X_test)
    metrics = model.evaluate(y_test)
    
    assert 'f1_score' in metrics
    assert 0 <= metrics['f1_score'] <= 1
```

### 8.6. Git Best Practices

**Commit Messages:**
```bash
âœ… GOOD
git commit -m "fix: normalize only Amount, not V1-V28 (align with paper)"

âŒ BAD
git commit -m "fix bug"
```

**Branch Strategy:**
```
main           â†’ Production-ready code
develop        â†’ Development branch
feature/smote  â†’ Feature branches
fix/normalize  â†’ Bugfix branches
```

---

## 9. Káº¾T LUáº¬N

### 9.1. Tá»•ng káº¿t kiáº¿n thá»©c Ä‘Ã£ há»c

**Machine Learning:**
- âœ… Binary Classification
- âœ… Supervised Learning
- âœ… Imbalanced Data Handling
- âœ… Model Evaluation Metrics
- âœ… Bias-Variance Tradeoff

**Algorithms:**
- âœ… Logistic Regression (Linear)
- âœ… Decision Tree (Non-linear)
- âœ… Naive Bayes (Probabilistic)
- âœ… Random Oversampling
- âœ… SMOTE

**Engineering:**
- âœ… Data Pipeline Design
- âœ… Design Patterns (Factory, Strategy, Template)
- âœ… Web Application (Streamlit)
- âœ… Code Organization
- âœ… Documentation

**Best Practices:**
- âœ… Train/Test Split Strategy
- âœ… Stratified Sampling
- âœ… Feature Engineering
- âœ… Model Comparison
- âœ… Production Deployment

### 9.2. Äiá»ƒm khÃ¡c biá»‡t giá»¯a Research vÃ  Production

| Aspect | Research Paper | Production Demo |
|--------|---------------|-----------------|
| **Focus** | Novel algorithm | User experience |
| **Code** | Jupyter notebooks | Modular, reusable |
| **Metrics** | Academic rigor | Business value |
| **UI** | Plots in paper | Interactive web app |
| **Docs** | Paper itself | Code + README |
| **Reproducibility** | Reported numbers | Runnable code |

### 9.3. Lessons Learned

**1. Alignment vá»›i Paper:**
- Äá»c ká»¹ paper Ä‘á»ƒ implement Ä‘Ãºng pipeline
- Normalize chá»‰ Amount, khÃ´ng pháº£i V1-V28
- Drop Time column nhÆ° mÃ´ táº£
- Reproduce káº¿t quáº£ chÃ­nh xÃ¡c

**2. Production Considerations:**
- User-friendly interface (Streamlit)
- Clear documentation
- Error handling
- Performance optimization
- Extensibility (easy to add new models)

**3. Trade-offs:**
- Accuracy vs Interpretability
- Precision vs Recall
- Speed vs Performance
- Simplicity vs Flexibility

### 9.4. Next Steps

**Äá»ƒ cáº£i thiá»‡n demo:**
1. ThÃªm ROC-AUC curve visualization
2. Implement model persistence (save/load)
3. Add hyperparameter tuning
4. Deploy to cloud (Streamlit Cloud, Heroku)
5. Add more models (Random Forest, XGBoost)
6. Implement cost-sensitive learning
7. Add feature importance analysis
8. Create API endpoint

---

## ðŸ“š TÃ€I LIá»†U THAM KHáº¢O

1. **Dataset**: [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)
2. **SMOTE Paper**: Chawla et al. (2002) - "SMOTE: Synthetic Minority Over-sampling Technique"
3. **Imbalanced Learning**: He & Garcia (2009) - "Learning from Imbalanced Data"
4. **Scikit-learn**: [Official Documentation](https://scikit-learn.org/)
5. **Streamlit**: [Official Documentation](https://docs.streamlit.io/)

---

**Document Version**: 1.0  
**Last Updated**: 31/10/2025  
**Author**: Fraud Detection Team
